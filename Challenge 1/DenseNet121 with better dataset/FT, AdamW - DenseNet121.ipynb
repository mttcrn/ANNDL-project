{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuwVgG3Vbbka"
   },
   "source": [
    "# ANNDL: 1st challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## ‚öôÔ∏è Import Libraries\n",
    "Import the libraries needed for the project and fix the seed for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "CO6_Ft_8T56A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gc #garbage collector\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "input_img_size = 96\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## ‚è≥ Load the Data\n",
    "\n",
    "The data are loaded from the filtered, pre-processed data. For more details, see '''Preprocessing.ipynb''' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLaoDaG1V1Yg"
   },
   "outputs": [],
   "source": [
    "filtered_dataset_path = '../Preprocessing/balanced_dataset.npz'\n",
    "data = np.load(filtered_dataset_path)\n",
    "N = int(len(data['images']) * 0.85)\n",
    "X_train = data['images'][:N]\n",
    "y_train = data['labels'][:N]\n",
    "\n",
    "\n",
    "# Define a mapping of labels to their corresponding digit names\n",
    "labels = {0:'Basophil', 1:'Eosinophil', 2:'Erythroblast', 3:'Immature granulocytes', 4:'Lymphocyte', 5:'Monocyte', 6:'Neutrophil', 7:'Platelet'}\n",
    "\n",
    "# Save unique labels\n",
    "unique_labels = list(labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to the range [0, 1] and encode output labels\n",
    "X_train = (X_train / 255).astype('float32')\n",
    "y_train = tfk.utils.to_categorical(y_train, num_classes=len(unique_labels))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=seed, stratify=y_train)\n",
    "print(f'Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}')\n",
    "print(f'Training labels shape: {y_train.shape}, Validation labels shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPzBQgMBo4SC"
   },
   "source": [
    "## üîÜ Augmenting the training and validation set\n",
    "We use AutoContrast and RandomSaturation from the keras library for the training set. It enhances contrast adaptively and may help in highlighting subtle differences between cell types without altering structure. \n",
    "We added the agumented images to the training set & we applied some rotation, zoom and shift.\n",
    "\n",
    "We use RandAugment for the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GRzF4ltipAY7"
   },
   "outputs": [],
   "source": [
    "# Import keras\n",
    "import keras_cv as kcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement AutoContrast\n",
    "value_range = [0, 1]\n",
    "autocontrast = kcv.layers.AutoContrast(value_range)\n",
    "contrast_result = autocontrast({'images': X_train, 'labels': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RandomSaturation\n",
    "factor = [0, 1]\n",
    "randomsaturation = kcv.layers.RandomSaturation(factor, seed=seed)\n",
    "saturation_result = randomsaturation({'images': X_train, 'labels': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Solarization\n",
    "value_range = [0, 1]\n",
    "solar = kcv.layers.Solarization(value_range)\n",
    "sol_result = solar({'images': X_train, 'labels': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement CutOut\n",
    "cutout_layer = kcv.layers.RandomCutout(height_factor=0.5, width_factor=0.5, seed=seed)\n",
    "cutout_result = cutout_layer({'images': X_train, 'labels': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, contrast_result[\"images\"]), axis=0)\n",
    "y_train = np.concatenate((y_train, contrast_result[\"labels\"]), axis=0)\n",
    "\n",
    "del contrast_result\n",
    "gc.collect()\n",
    "\n",
    "X_train = np.concatenate((X_train, saturation_result[\"images\"]), axis=0)\n",
    "y_train = np.concatenate((y_train, saturation_result[\"labels\"]), axis=0)\n",
    "\n",
    "del saturation_result\n",
    "gc.collect() \n",
    "\n",
    "X_train = np.concatenate((X_train, cutout_result[\"images\"]), axis=0)\n",
    "y_train = np.concatenate((y_train, cutout_result[\"labels\"]), axis=0)\n",
    "\n",
    "del cutout_result\n",
    "gc.collect()\n",
    "\n",
    "X_train = np.concatenate((X_train, sol_result[\"images\"]), axis=0)\n",
    "y_train = np.concatenate((y_train, sol_result[\"labels\"]), axis=0)\n",
    "\n",
    "del sol_result\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting validation set\n",
    "randaugment = kcv.layers.RandAugment(value_range=(0,1), augmentations_per_image=3, magnitude=0.3)\n",
    "val_augmented = randaugment(X_val)\n",
    "print(\"val_agumented shape:\", val_augmented.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,         # Equivalent to RandomRotation\n",
    "    width_shift_range=0.2,     # Equivalent to RandomTranslation on x-axis\n",
    "    height_shift_range=0.2,    # Equivalent to RandomTranslation on y-axis\n",
    "    zoom_range=0.2,            # Equivalent to RandomZoom\n",
    "    horizontal_flip=True,      # Equivalent to RandomFlip (horizontal)\n",
    "    vertical_flip=True         # Equivalent to RandomFlip (vertical)\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in .zip file training and validation set\n",
    "np.savez_compressed('aug_steroid_dataset', images_t=X_train, labels_t=y_train, images_v = val_augmented, labels_v = y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise densenet model with pretrained weights, for transfer learning\n",
    "dense = tfk.applications.densenet.DenseNet121(\n",
    "    input_shape=(96, 96, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling=None,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "# dense.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in DenseNet to use it solely as a feature extractor\n",
    "dense.trainable = False\n",
    "\n",
    "# Define input layer with shape matching the input images\n",
    "inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')\n",
    "\n",
    "# Preprocess inputs for DenseNet\n",
    "x = tfk.applications.densenet.preprocess_input(inputs)\n",
    "\n",
    "# Pass inputs through the feature extractor\n",
    "x = dense(inputs)\n",
    "\n",
    "# Add a Global Average Pooling layer to flatten the spatial dimensions\n",
    "x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "\n",
    "# Add a dropout layer for regularisation\n",
    "x = tfkl.Dropout(0.5, name='dropout')(x)\n",
    "\n",
    "# Add final Dense layer for classification with L2 regularization\n",
    "outputs = tfkl.Dense(8, activation='softmax', name='output')(x)\n",
    "\n",
    "# Define the complete model linking input and output\n",
    "tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and AdamW optimiser\n",
    "optimizer = tfk.optimizers.AdamW()\n",
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "# tl_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to stop the training when val_accuracy stop to increase\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Reduce LR when val_loss stop to decrease to avoid local minumum\n",
    "reduce_lr_on_plateau = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.7,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "# ModelCheckpoint save the best weights along the training \n",
    "checkpoint = tfk.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',  \n",
    "    monitor='val_loss',        \n",
    "    save_best_only=True        \n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr_on_plateau, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tl_history = tl_model.fit(\n",
    "    train_generator,\n",
    "    batch_size=16,\n",
    "    epochs=6,\n",
    "    validation_data=(val_augmented, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Calculate and print the best validation accuracy achieved\n",
    "final_val_accuracy = round(max(tl_history['val_accuracy']) * 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file, including final accuracy in the filename\n",
    "model_filename = 'TL_dense_' + str(final_val_accuracy) + '.keras'\n",
    "tl_model.save(model_filename)\n",
    "\n",
    "# Free memory by deleting the model instance\n",
    "del tl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the best model after transfer learning\n",
    "ft_model = tfk.models.load_model(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.get_layer('densenet121').trainable = True\n",
    "\n",
    "# Set all DenseNet121 layers as non-trainable\n",
    "for layer in ft_model.get_layer('densenet121').layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Enable training only for Conv2D layers\n",
    "for i, layer in enumerate(ft_model.get_layer('densenet121').layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        layer.trainable = True\n",
    "        print(i, layer.name, type(layer).__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of layers to freeze\n",
    "N = 100\n",
    "\n",
    "# Set the first N layers as non-trainable\n",
    "for i, layer in enumerate(ft_model.get_layer('densenet121').layers[:N]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# Print layer indices, names, and trainability status\n",
    "for i, layer in enumerate(ft_model.get_layer('densenet121').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "#ft_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowering the learning rate for fine-tuning\n",
    "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(0.0001), metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_on_plateau = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=0.0\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "ft_history = ft_model.fit(\n",
    "    train_generator, \n",
    "    batch_size = 32,\n",
    "    epochs = 20,\n",
    "    validation_data = (val_augmented, y_val),\n",
    "    callbacks = callbacks\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(ft_history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'weights.keras'\n",
    "ft_model.save(model_filename)\n",
    "\n",
    "# Delete the model to free up resources\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSliIxBvbs2Q"
   },
   "source": [
    "## Evaluate and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8exErcE-rin"
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = tfk.models.load_model(model_filename)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "predictions = model.predict(val_augmented)\n",
    "pred_classes = np.argmax(predictions, axis=-1)\n",
    "true_classes = np.argmax(y_val, axis=-1)\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm.T, annot=True, fmt='', xticklabels=unique_labels, yticklabels=unique_labels, cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3LrBOUIBQcg"
   },
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy metrics\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(tl_history['loss'], label='Training set', alpha=.3, color='green', linestyle='--')\n",
    "plt.plot(tl_history['val_loss'], label='Validation set', alpha=.8, color='blue')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(tl_history['accuracy'], label='Training set', alpha=.3, color='green', linestyle='--')\n",
    "plt.plot(tl_history['val_accuracy'], label='Validation set', alpha=.8, color='blue')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNp6pUZuddqC"
   },
   "source": [
    "## üìä Prepare Your Submission\n",
    "\n",
    "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
    "\n",
    "```python\n",
    "# file: model.py\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKT4h-9xYwiT"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the internal state of the model. Note that the __init__\n",
    "        method cannot accept any arguments.\n",
    "\n",
    "        The following is an example loading the weights of a pre-trained\n",
    "        model.\n",
    "        \"\"\"\n",
    "        self.neural_network = tfk.models.load_model('weights.keras')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
    "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
    "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
    "        encoded.\n",
    "\n",
    "        The following is an example of a prediction from the pre-trained model\n",
    "        loaded in the __init__ method.\n",
    "        \"\"\"\n",
    "        X = X / 255.0\n",
    "        preds = self.neural_network.predict(X)\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the .zip file to be submitted\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if model.py and weights.keras exist\n",
    "if os.path.exists(\"model.py\") and os.path.exists(\"weights.keras\"):\n",
    "    # Create a ZIP file containing model.py and weights.keras\n",
    "    with zipfile.ZipFile(\"sub.zip\", \"w\") as model_zip:\n",
    "        model_zip.write(\"model.py\")\n",
    "        model_zip.write(\"weights.keras\")\n",
    "    print(\"model.zip has been created successfully.\")\n",
    "else:\n",
    "    print(\"model.py or weights.keras not found. Please ensure both files are in the current directory.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GN_cpHlSboXV",
    "RNp6pUZuddqC"
   ],
   "gpuType": "T4",
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
